<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>How [not] to evaluate your RAG</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/dracula.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>HOW <s>NOT</s> TO EVALUATE</h2>
				<img src="img/main.png" height="350px" style="margin: 0px;">
				<h1>your RAG</h1>

				<small>Berlin Buzzwords 2025 | Roman Grebennikov</small>
			</section>
			<section>
				<h2>whoami</h2>
				<p><img src="img/ava.jpg" height="200px"></p>
				<ul>
					<li>PhD in CS, quant trading, credit scoring</li>
					<li><strong>Findify</strong>: e-commerce search, personalization</li>
					<li><strong>Delivery Hero</strong>: food search, LLMs</li>
					<li><strong>Opensource</strong>: Metarank, lightgbm4j, flink-scala-api</li>
				</ul>
			</section>
			<section>
				<h2>Nixiesearch</h2>
				<img src="img/arch.png" height="200px">
				<ul>
					<li><strong>TLDR:</strong> Lucene search engine on top of S3</li>
					<li><strong>Expected:</strong> facets, filters, autocomplete, RRF</li>
					<li><strong>ML:</strong> embedding inference, cross-encoder ranking</li>
					<li><strong>RAG:</strong> LLM inference via llamacpp</li>
				</ul>
			</section>
			<section>
				<h2>Perks of being open-source</h2>
				<br />
				<ul>
					<li class="fragment"><strong>they</strong>: hey nice project!</li>
					<li class="fragment"><strong>you</strong>: thanks! ‚ù§Ô∏è</li>
					<li class="fragment"><strong>they</strong>: we use it for RAG in a bank with 3M customers</li>
					<li class="fragment"><strong>they</strong>: and got some issues, can you help?</li>
					<li class="fragment"><strong>you</strong>: hmm ü§îü§îü§î</li>
				</ul>
			</section>
			<section>
				<h2>The agenda</h2>
				<img src="img/bitch.png" height="300px">
				<ul>
					<li class="fragment"><strong>intro</strong>: RAG as a support agent for support agents</li>
					<li class="fragment"><strong>data</strong>: chunking and context length</li>
					<li class="fragment"><strong>search+generation</strong>: the R and G in RAG</li>
					<li class="fragment">
						<strong>tools</strong>: RAGAS, and why you should perhaps build your own
					</li>
				</ul>
			</section>
			<section>
				<h2>Support agent for support agents</h2>
				<p class="fragment">80% of all questions are covered by FAQ</p>
				<img src="img/balance.png" height="200px" class="fragment">
				<ul class="fragment">
					<li><strong>40%</strong>: answer immediately as you know the answer</li>
					<li><strong>40%</strong>: answer after checking FAQ/docs</li>
					<li><strong>20%</strong>: answer after internal discussion</li>
				</ul>
			</section>
			<section>
				<h2>Support agent for support agents</h2>
				<img src="img/bot-idea.png" height="400px">
				<ul>
					<li><strong>Still human</strong>: nobody likes chatting with ChatGPT</li>
					<li><strong>Faster onboarding</strong>: just read the docs</li>
				</ul>
			</section>
			<section>
				<h2>RAG</h2>
				<ul>
					<li><strong>Dialogue</strong>: summarize to a query</li>
					<li><strong>Retrieve</strong>: search for top-N relevant docs</li>
					<li><strong>Summarize</strong>: answer the last question in the dialogue</li>
				</ul>
				<img src="img/bot-idea.png" height="400px">
			</section>
			<section>
				<h2>Getting real</h2>
				<img src="img/uzbek.png" height="350px">
				<ul>
					<li class="fragment"><strong>FinTech</strong>: airgapped, you can't just use OpenAI API</li>
					<li class="fragment"><strong>Languages</strong>: CIS region, Uzbek/Kazakh</li>
					<li class="fragment"><strong>Knowledge base</strong>: what knowledge base?</li>
				</ul>
			</section>
			<section>
				<h2>wait is RAG solved thing in 2025?</h2>
				<img src="img/rags.png" height="400px">
			</section>
			<section>
				<video data-autoplay src="video/catrag2.mp4"></video>
			</section>
			<section>
				<h2>Iteration #1</h2>
				<ul>
					<li>LLM convert all docs to Markdown</li>
					<li>LangChain chunk, embed with multilingual-e5-small</li>
					<li>Qwen2.5 for summarization</li>
				</ul>
				<img src="img/ctok.png" height="400px" class="fragment">
			</section>
			<section>
				<h2>CTO@k: it works but sucks</h2>
				<ul>
					<li>Relevant docs missing, irrelevant found</li>
					<li>Wrong query (or summary) generated</li>
				</ul>
			</section>
			<section>
				<h2>CTO@k: it works but sucks</h2>
				<ul>
					<li><strong>R: </strong>Relevant docs missing, irrelevant found</li>
					<li><strong>G: </strong>Wrong query (or summary) generated</li>
				</ul>
			</section>
			<!--section>
				<h2>RAG in a nutshell</h2>
				<img src="img/rag-nut.png" height="400px">
			</section-->
			<section>
				<h2>RAG as a system</h2>
				<p>preprocessing + retrieval + generation = RAG</p>
				<img src="img/men.png" height="500px" style="margin: 0px;" class="fragment">
			</section>
			<section>
				<h2>Vibe-evaluating RAG</h2>
				<img src="img/ragas.png" height="400px">
				<p>TLDR: evaluate each RAG step separately</p>
			</section>
			<section>
				<h2>decompose > evaluate > improve</h2>
			</section>
			<section>
				<h2>Corpus preprocessing</h2>
				<img src="img/incredible.png" height="500px">
			</section>
			<section>
				<h2>Corpus preprocessing</h2>
				<img src="img/incredible.png" height="300px">
				<ul>
					<li>Local files: docx, pdf, txt, napkin scans</li>
					<li>Convert everything to markdown</li>
				</ul>
			</section>
			<section>
				<h2>Evaluating chunking</h2>
				<p><img src="img/use_your_product.jpg" height="350px" style="margin: 0px;"></p>
				<ul>
					<li>chunking = the way you create corpus</li>
					<li>cannot label changing corpus üôÅ</li>
				</ul>
				<p>"vibe fixing"</p>
			</section>
			<section>
				<h2>Chunking: large vs small</h2>
				<ul>
					<li><strong>Large</strong>: embed complete documents</li>
					<li><strong>Small</strong>: split docs to chunks</li>
				</ul>
			</section>
			<section>
				<h2>Problems of large chunking</h2>
				<img src="img/books.png" height="450px">
				<p>Bad UX: no time to check, context too large</p>
			</section>
			<section>
				<h2>Problems of small chunking</h2>
				<img src="img/chunks.png" height="300px">
				<p>Lost context due to over-chunking</p>
			</section>
			<section>
				<h2>Anthropic's contextual chunking</h2>
				<img src="img/context.png">
			</section>
			<section>
				<h2>Anthropic's contextual chunking</h2>
				<img src="img/context.png" height="300px">
				<ul>
					<li>LLM inference per chunk is expensive üôÅ</li>
					<li>Yes but we have markdown titles üòÉ</li>
				</ul>
			</section>
			<section>
				<h2>GPU poor contextual chunking</h2>
				<img src="img/context-chunks.png" height="500px">
			</section>
			<section>
				<h2>Chunking TLDR</h2>
				<img src="img/win.png" height="450px">
				<ul>
					<li>Contextual: title + 1-2 paragraphs</li>
				</ul>
			</section>
			<section>
				<h2>Immutable index</h2>
				<img src="img/s3-index.png" height="250px" style="margin:0px;">
				<pre><code data-trim data-noescape class="language-bash">
$> nixiesearch index file -c config.yml --url s3://bucket/source --index hello

$> aws s3 ls s3://bucket/index/hello
-rw-r--r-- 1 shutty shutty       512 May 22 12:55 _0.cfe
-rw-r--r-- 1 shutty shutty 123547444 May 22 12:55 _0.cfs
-rw-r--r-- 1 shutty shutty       322 May 22 12:55 _0.si
-rw-r--r-- 1 shutty shutty      1610 May 22 12:55 index.json
-rw-r--r-- 1 shutty shutty       160 May 22 12:55 segments_1
-rw-r--r-- 1 shutty shutty         0 May 22 12:48 write.lock

$> nixiesearch search -c config.yml
				</code></pre>
			</section>
			<section>
				<h2>R in RAG</h2>
				<img src="img/ragas-metrics.png" height="450px">
			</section>
			<section>
				<h2>R in RAG</h2>
				<img src="img/search-metrics.png" height="500px">
			</section>
			<section>
				<h2>The most relevant context</h2>
				<img src="img/rag-nut.png" height="300px">
				<ul>
					<li>Context window is limited (but it's growing)</li>
					<li>Fill the context with relevant chunks</li>
				</ul>
			</section>
			<section>
				<h2>Which model to choose?</h2>
				<p>Not all embeddings are OK for non-English!</p>
				<ul>
					<li>No labels, no queries, unusual languages</li>
					<li>No baseline to compare against</li>
					<li>No public eval corpus üôÅ</li>
				</ul>
			</section>
			<section>
				<h2>MIRACL dataset</h2>
				<img src="img/miracl.png" height="400px">
				<ul>
					<li>TLDR: wikipedia query + docs + labels, 18 languages</li>
					<li>No Kazakh/Uzbek split, but what if we machine-translate? ü§î</li>
				</ul>
			</section>
			<section>
				<h2>Machine-translated datasets?</h2>
				<p>m-MARCO: A Multilingual Version of the MS MARCO [2021]</p>
				<img src="img/mmarco.png" height="350px">
				<p>Are MT/LLM models of 2025 better? ü§î</p>
			</section>
			<section>
				<h2>XCOMET, en->xx</h2>
				<img src="img/xcomet.png" height="400px">
				<ul>
					<li>Google Translate: great on high-resource languages</li>
					<li>GPT4o: strong on low-resource languages</li>
				</ul>
			</section>
			<section>
				<h2>MTEB eval on MT data</h2>
				<img src="img/embeds.png">
				<ul>
					<li>The bigger = the better on low-resource languages</li>
					<li>Wait, LLM as an embedding model?</li>
				</ul>
			</section>
			<section>
				<h2>Decoder-only embeddings</h2>
				<img src="img/decoder.png" height="400px" style="margin: 0px;">
				<ul>
					<li>BERT embeddings: bidirectional attention</li>
					<li>Decoder embeddings: only last token knows it all</li>
					<li>Decoders: prefill+generation stages</li>
				</ul>
			</section>
			<section>
				<img src="img/qwen3.png" height="550px">
			</section>
			<section>
				<h2>LLM embeddings and ONNX</h2>
				<pre><code class="language-python">
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("Qwen/Qwen3-Embedding-8B", backend="onnx")

# raw export
model.save_pretrained("export_dir")

# optimization - DOES NOT WORK
export_optimized_onnx_model(model, "O3", "export_dir")

# QInt8 quantization
export_dynamic_quantized_onnx_model(model, "avx512_vnni", "export_dir")
				</code></pre>
			</section>
			<section>
				<h2>ONNX + Nixiesearch</h2>
				<pre><code data-trim data-noescape class="language-yaml" style="max-height: fit-content;">
inference:
  embedding:
    # our embedding model
    bge-gemma:
      provider: onnx
      model: file://dir/model_qint8.onnx

schema:
  # index schema
  knowledgebase:
    fields:
      content:
        type: text
        search:
          semantic:
            model: bge-gemma
</code></pre>
				<ul>
					<li>just worked, CLS pooling, even qint8 quantization</li>
				</ul>
			</section>
			<section>
				<h2>okey retrieval = quepid time?</h2>
				<ul>
					<li><strong>Perfect world</strong>: get queries, label results</li>
					<li><strong>Reality</strong>: "who's going to label results?"</li>
				</ul>
				<p>RAGAS approach: LLM-as-a-judge</p>
			</section>
			<section>
				<h2>Context precision+recall</h2>
				<img src="img/precision-recall.png" height="500px" style="margin: 0px;">
				<p>No NDCG: position does not matter for LLM</p>
			</section>
			<section>
				<h2>Context relevance</h2>
				<img src="img/precision-recall-emb.png" height="550px" style="margin: 0px;">
			</section>
			<section>
				<h2>Context retrieval metrics overview</h2>
				<ul>
					<li>Absolute numbers: not much benefit</li>
					<li>Relative performance: with automated evals!</li>
				</ul>
			</section>
			<section>
				<h2>G in RAG</h2>
				<img src="img/rag-nut.png" height="300px">
				<ul>
					<li>What open LLM to use?</li>
					<li>Balance: precision, hardware, latency</li>
				</ul>
			</section>
			<section>
				<h2>Evaluating generation</h2>
				<img src="img/rag-diag.png" height="450px" style="margin: 0px;">
				<ul>
					<li><strong>Comprehension</strong>: how LLM can understand language?</li>
					<li><strong>Generation</strong>: how good is generated response?</li>
				</ul>
			</section>
			<section>
				<h2>FB Belebele benchmark</h2>
				<img src="img/belebele.png" height="450px" style="margin: 0px;">
				<ul>
					<li>122 languages: passage, question, 4 answers</li>
					<li>400 triplets per language</li>
				</ul>
			</section>
			<section>
				<h2>Task example</h2>
				<pre><code>
Passage:
The atom can be considered to be one of the fundamental building blocks of 
all matter. Its a very complex entity which consists, according to a simplified 
Bohr model, of a central nucleus orbited by electrons, somewhat similar to 
planets orbiting the sun - see Figure 1.1. The nucleus consists of two 
particles - neutrons and protons. Protons have a positive electric charge 
while neutrons have no charge. The electrons have a negative electric charge.
	
Query: The particles that orbit the nucleus have which type of charge?
	
A: Positive charge
B: No charge
C: Negative charge
D: Positive and negative charge					
				</code></pre>
				<ul>
					<li>TLDR: NLU evaluation, but multilingual</li>
				</ul>
			</section>
			<section>
				<h2>Results</h2>
				<img src="img/belebele-results.jpg" style="margin: 0px;">
				<p>TLDR: Gemma2 (and 3!) is one of the best open LLMs</p>
			</section>
			<section>
				<h2>Benchmarking generation</h2>
				<img src="img/perplexity.webp">
				<p><small>[1]: <a
							href="https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/">UpTrain:
							Decoding Perplexity and its significance in LLMs</a></small></p>
			</section>
			<section>
				<h2>Perplexity</h2>
				<img src="img/perp-results.jpg" style="margin: 0px;">
				<ul>
					<li>Gemma2-27B: balance of comprehension+generation</li>
					<li>Later upgrade to Gemma3-27B</li>
				</ul>
			</section>
			<section>
				<h2>Uncharted territory</h2>
				<p>LLM-as-a-judge ü§îü§îü§î metrics which do exist, but never tried:</p>
				<ul>
					<li><strong>Answer accuracy</strong>: compare prediction and ground truth</li>
					<li><strong>Answer groundness</strong>: is answer based on context?</li>
				</ul>
			</section>
			<section>
				<h2>RAGAS: answer accuracy</h2>
				<img src="img/accuracy.png" height="550px">
			</section>
			<section>
				<h2>RAGAS: answer groundness</h2>
				<img src="img/ground.png" height="550px">
			</section>
			<section>
				<img src="img/ragas-metrics2.png">
			</section>
			<section>
				<h2>RAGAS vs client.completion</h2>
				<ul>
					<li>ended up with custom prompt collection</li>
					<li>
						bad prompt example:
						<pre><code>
is document '{doc}' relevant to query '{query}'?
						</code></pre>
					</li>
					<li>
						good prompt example:
						<pre><code>
classify '{doc}' relevance to query '{query}':                    
score 0: doc has different topic and don't answer 
         the question asked in the query.
score 1: doc has the same or similar topic, but don't 
         answer the the query.
score 2: document exactly answers the question in query
						</code></pre>

					</li>
				</ul>
			</section>
			<section>
				<h2>Implementation</h2>
				<img src="img/click.png">
				<ul>
					<li>on-prem: 1-2x GPU, 4U server</li>
					<li>perf: 100 tps, 500ms time to first token</li>
				</ul>
			</section>
			<section>
				<h2>Nixiesearch, llamacpp, GGUF</h2>
				<pre><code data-trim data-noescape class="language-yaml" style="max-height: fit-content;">
inference:
  completion:
    # LLM inference here!
    gemma2-27b:
	    provider: llamacpp
	    model: file://dir/model_Q4_K_0.GGUF
  embedding:
    bge-gemma:
      provider: onnx
      model: file://dir/model_qint8.onnx

schema:
  knowledgebase:
    fields:
      content:
        type: text
        search:
          semantic:
            model: bge-gemma
</code></pre>
			</section>
			<section>
				<h2>Things we learned</h2>
				<ul>
					<li>English >> High-resource >> Low-resource languages</li>
					<li><strong>Decompose</strong>: complex system = many simple parts</li>
					<li><strong>LLM-as-a-judge</strong>: when you have no choice</li>
				</ul>
			</section>
			<section>
				<h2>Links</h2>
				<img src="img/rickroll.png" height="300px">
				<small>
					<ul>
						<li><strong>RAGAS</strong>: <a href="https://www.ragas.io/">https://www.ragas.io/</a></li>
						<li><strong>Contextual Chunking</strong>: <a
								href="https://www.anthropic.com/news/contextual-retrieval">https://www.anthropic.com/news/contextual-retrieval</a>
						</li>
						<li><strong>FB Belebele benchmark</strong>: <a
								href="https://huggingface.co/datasets/facebook/belebele">https://huggingface.co/datasets/facebook/belebele</a>
						</li>
						<li><strong>Nixiesearch</strong>: <a
								href="https://github.com/nixiesearch/nixiesearch">https://github.com/nixiesearch/nixiesearch</a>
						</li>
					</ul>
				</small>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			history: true,
			controls: true,
			progress: true,
			width: 1200,
			transition: 'none',
			slideNumber: true,
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>
</body>

</html>